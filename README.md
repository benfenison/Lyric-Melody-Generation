# AI-generated rock songs
With thousands of distinctively different sounding rock songs produced up to now, the genre of rock music has reinvented itself over the decades. To participate in the never-ending evolution of rock music, we decided to explore the use of AI to generate uniquely new rock songs using GPT-2 for lyrics generation and TeleMelody for melody generation. 
## 1.Data Collection

## 2.Lyrics Generation
The lyrics generation step requires a csv file containing a single column with rows of different song lyrics, 
and the gpt2 pre-trained model which can be downloaded using the command `gpt_2_simple.download_gpt2(model_name="124M")`.

The training data we used to fine-tune the GPT-2 model can be downloaded [here](https://drive.google.com/drive/folders/1zv6nbWG1YDBpUDHKIJJuUCeksbs9q7nN?usp=sharing), along with a fine-tuned checkpoint file.

### 2.1 Model fine-tuning
To fine-tune a model, call the `GPT2_lyrics_train.py` file in the command line and provide the path to the csv file.

```python GPT2_lyrics_train.py "<file path>"```

**Some useful fine-tuning model parameters:**
* **model_name**: locally downloaded pre-trained model
* **learning_rate**: tuning parameter for the step size at each iteration
* **optimizer**: either 'adam' or 'sgd'
* **batch_size**: number of samples for each iteration
* **steps**: number of iterations to train
* **sample_every**: generate samples every n steps
* **sample_length**: print text with n tokens
* **save_every**: write a checkpoint every n steps
* **print_every**: print training progress
* **restore_from**: either "fresh" to start training from the default pre-trained model or "latest" to continue training from a checkpoint file
* **run_name**: fine-tuned model name

This will create a checkpoint for this fine-tuned model which can be used during the lyrics generation step.

### 2.2 Generating the lyrics
The lyrics can be generated by calling the `GPT2_lyrics_generate.py` file in the command line and provide the path to the fine-tuned checkpoint folder.

```python GPT2_lyrics_generate.py "<checkpoint folder path>"```

**Some useful parameters to generate lyrics:**
* **temperature**: higher temperature the model gives more random text generations (default=0.85)
* **top_p**: cumulative probability to control the number of word guesses (default=0.9)
* **top_k**: top k number of words to guess from when generating (default=0 i.e. disabled feature)
* **length**: number of tokens to generate (default=200. max token allowed is 1023)
  
## 3.Lyric to Melody Generation
### 3.1 Preprocessing Lyrics
Preprocessing the lyrics requires for the lyrics to be in the most basic level of speech, which is a phoneme. The combination of phonemes create words.

``` 
! mkdir data/
```

``` 
From Phonemization import get_phones
get_phones('lyrics.txt')
```
### 3.2 Lyrics to Melody

- Obtain lyrics2rhythm Checkpoints from [checkpoint_best.pt](https://msramllasc.blob.core.windows.net/modelrelease/lyric2rhythm_en_best.pt)
- Obtain Template2melody Checkpoints from [checkpoint_best.pt](https://msramllasc.blob.core.windows.net/modelrelease/template2melody_best.pt)

Place each checkpoint in its respective folder, and remove placeholder checkpoint in ---->  [Input checkpoints Here](https://github.com/benfenison/Lyric-Melody-Generation/tree/main/Lyric-to-Melody/checkpoints)

Input lyrics.txt file & syllables.txt into this [repository](https://github.com/benfenison/Lyric-Melody-Generation/tree/main/Lyric-to-Melody/data/en/test)
Edit Chord Progression at discretion

```
! cd Lyric-Melody-Generation/Lyric-to-Melody/
! python infer_en.py lyric2rhythm template2melody test output
```
