{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from gensim import corpora, models\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the lyrics csv which have already been tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>words</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Planet Zero</td>\n",
       "      <td>Shinedown</td>\n",
       "      <td>Planet Zero Lyrics[Verse 1]\\r\\nDown here on pl...</td>\n",
       "      <td>[planet, zero, lyric, planet, zero, swing, gav...</td>\n",
       "      <td>[([Verse 1],  Down here on planet zero They sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Black Summer</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>Black Summer Lyrics[Verse 1]\\r\\nA lazy rain am...</td>\n",
       "      <td>[black, summer, lyric, lazy, rain, sky, refuse...</td>\n",
       "      <td>[([Verse 1],  A lazy rain am I, the skies refu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Love Dies Young</td>\n",
       "      <td>Foo Fighters</td>\n",
       "      <td>Love Dies Young Lyrics[Verse 1]\\r\\nLove dies y...</td>\n",
       "      <td>[love, dy, young, lyric, love, dy, young, resu...</td>\n",
       "      <td>[([Verse 1],  Love dies young and there's no r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>So Called Life</td>\n",
       "      <td>Three Days Grace</td>\n",
       "      <td>So Called Life Lyrics[Verse 1]\\r\\nCan't laugh,...</td>\n",
       "      <td>[called, life, lyric, laugh, cry, live, die, a...</td>\n",
       "      <td>[([Verse 1],  Can't laugh, can't cry, can't li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>For The Glory (feat. Hollywood Undead)</td>\n",
       "      <td>All Good Things</td>\n",
       "      <td>For the Glory Lyrics[Verse 1]\\r\\nBetter back d...</td>\n",
       "      <td>[glory, lyric, better, back, domain, got, whol...</td>\n",
       "      <td>[([Verse 1],  Better back down, you're in my d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                   track                 artist  \\\n",
       "0           0                             Planet Zero              Shinedown   \n",
       "1           1                            Black Summer  Red Hot Chili Peppers   \n",
       "2           2                         Love Dies Young           Foo Fighters   \n",
       "3           3                          So Called Life       Three Days Grace   \n",
       "5           5  For The Glory (feat. Hollywood Undead)        All Good Things   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  Planet Zero Lyrics[Verse 1]\\r\\nDown here on pl...   \n",
       "1  Black Summer Lyrics[Verse 1]\\r\\nA lazy rain am...   \n",
       "2  Love Dies Young Lyrics[Verse 1]\\r\\nLove dies y...   \n",
       "3  So Called Life Lyrics[Verse 1]\\r\\nCan't laugh,...   \n",
       "5  For the Glory Lyrics[Verse 1]\\r\\nBetter back d...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [planet, zero, lyric, planet, zero, swing, gav...   \n",
       "1  [black, summer, lyric, lazy, rain, sky, refuse...   \n",
       "2  [love, dy, young, lyric, love, dy, young, resu...   \n",
       "3  [called, life, lyric, laugh, cry, live, die, a...   \n",
       "5  [glory, lyric, better, back, domain, got, whol...   \n",
       "\n",
       "                                            segments  \n",
       "0  [([Verse 1],  Down here on planet zero They sw...  \n",
       "1  [([Verse 1],  A lazy rain am I, the skies refu...  \n",
       "2  [([Verse 1],  Love dies young and there's no r...  \n",
       "3  [([Verse 1],  Can't laugh, can't cry, can't li...  \n",
       "5  [([Verse 1],  Better back down, you're in my d...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the lyrics as dataframe\n",
    "country_lyrics_df = pd.read_csv('Data\\df_cty_lyrics.csv', converters={'words':literal_eval,'segments':literal_eval})\n",
    "country_lyrics_df = country_lyrics_df.drop_duplicates('track')\n",
    "country_lyrics_df.head()\n",
    "\n",
    "rock_lyrics_df = pd.read_csv('Data\\df_rock_lyrics.csv', converters={'words':literal_eval,'segments':literal_eval})\n",
    "rock_lyrics_df = rock_lyrics_df.drop_duplicates('track')\n",
    "rock_lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[buy, dirt, lyric, day, turned, 80, sitting, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[fancy, like, lyric, ayy, girl, bangin, low, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sand, boot, lyric, asked, said, somewhere, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[didnt, love, lyric, mind, bein, alone, keep, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[livin, dream, lyric, mama, pray, success, any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>[didnt, much, lyric, tonka, truck, gi, joes, j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>[makin, plan, lyric, ever, left, town, never, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[god, speed, album, version, lyric, one, go, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[mountain, lyric, got, spell, draggin, heart, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[happier, alone, lyric, ever, turn, light, eve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>983 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 words\n",
       "0    [buy, dirt, lyric, day, turned, 80, sitting, b...\n",
       "1    [fancy, like, lyric, ayy, girl, bangin, low, m...\n",
       "2    [sand, boot, lyric, asked, said, somewhere, ne...\n",
       "3    [didnt, love, lyric, mind, bein, alone, keep, ...\n",
       "8    [livin, dream, lyric, mama, pray, success, any...\n",
       "..                                                 ...\n",
       "993  [didnt, much, lyric, tonka, truck, gi, joes, j...\n",
       "994  [makin, plan, lyric, ever, left, town, never, ...\n",
       "995  [god, speed, album, version, lyric, one, go, w...\n",
       "996  [mountain, lyric, got, spell, draggin, heart, ...\n",
       "997  [happier, alone, lyric, ever, turn, light, eve...\n",
       "\n",
       "[983 rows x 1 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign the tokenzied words to tokenized_lyrics\n",
    "tokenized_lyrics = country_lyrics_df[['words']]\n",
    "tokenized_lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering of lyrics tokens into tf and tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy function for converting list of tokens into objects\n",
    "# so we don't have to rejoin the tokens into strings as inputs into the vectorizers\n",
    "def dummy_func(lyric):\n",
    "    return lyric\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=dummy_func, max_df=0.95, min_df=2, lowercase=False)\n",
    "tfidf = tfidf_vectorizer.fit_transform(list(tokenized_lyrics.words.values))\n",
    "\n",
    "count_vectorizer = CountVectorizer(tokenizer=dummy_func, max_df=0.95, min_df=2, lowercase=False)\n",
    "tf = count_vectorizer.fit_transform(list(tokenized_lyrics.words.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, n_top_words):\n",
    "    '''\n",
    "    Displays the top words for each topic.\n",
    "    Parameters:\n",
    "        model -> topic model from sklearn\n",
    "        feature_names -> list of str\n",
    "        n_top_words -> number of top words to show for each topic\n",
    "    '''\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "\n",
    "# no. of topics\n",
    "n_topics = 5\n",
    "# no. of top words to show for each topic\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=10).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "know like yeah got love one get girl time way\n",
      "Topic 1:\n",
      "bloom one woman hand na man time would stephen new\n",
      "Topic 2:\n",
      "like oh love ooh gonna little wanna yeah time get\n",
      "Topic 3:\n",
      "let love like got night go back one long get\n",
      "Topic 4:\n",
      "nigga bitch yeah got like get fuck shit gon as\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imhar\\anaconda3\\envs\\MADS\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, count_vectorizer.get_feature_names(), n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imhar\\anaconda3\\envs\\MADS\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1422: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_topics, random_state=1, \n",
    "            alpha=.1, \n",
    "            # l1_ratio=.5, \n",
    "            init='nndsvd').fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "love know never could say let one heart would want\n",
      "Topic 1:\n",
      "nigga bitch fuck got shit gon yeah get real ayy\n",
      "Topic 2:\n",
      "ooh oohooh oh ohoh know yeah yeahyeahyeah runnin care high\n",
      "Topic 3:\n",
      "yeah like got girl little good get back night country\n",
      "Topic 4:\n",
      "wanna baby make like take know feel kiss gonna stay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imhar\\anaconda3\\envs\\MADS\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf, tfidf_vectorizer.get_feature_names(), n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a BoW dictionary\n",
    "dictionary = corpora.Dictionary(tokenized_lyrics.words)\n",
    "dictionary.filter_extremes(no_below=10, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_freq_words(dictionary, n_top_words):\n",
    "    '''\n",
    "    Return the top words based on freq in the BoW dictionary.\n",
    "    Parameters:\n",
    "        dictionary -> bag-of-word count of word frequency\n",
    "        n_top_words -> n most frequent words in the dictionary\n",
    "    '''\n",
    "    for k, v in dictionary.iteritems():\n",
    "        print(k, v)\n",
    "        if k == n_top_words:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 add\n",
      "1 around\n",
      "2 back\n",
      "3 buy\n",
      "4 call\n",
      "5 caught\n",
      "6 chasing\n",
      "7 church\n",
      "8 coffee\n",
      "9 count\n",
      "10 day\n"
     ]
    }
   ],
   "source": [
    "top_freq_words(dictionary, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(lyric) for lyric in tokenized_lyrics.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = models.LdaMulticore(bow_corpus, num_topics=n_topics, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.015*\"one\" + 0.011*\"go\" + 0.011*\"time\" + 0.011*\"little\" + 0.009*\"let\" + 0.008*\"night\" + 0.008*\"back\" + 0.008*\"every\" + 0.007*\"right\" + 0.007*\"never\"\n",
      "Topic: 1 \n",
      "Words: 0.014*\"baby\" + 0.012*\"back\" + 0.011*\"need\" + 0.010*\"nigga\" + 0.009*\"time\" + 0.009*\"want\" + 0.009*\"go\" + 0.008*\"come\" + 0.008*\"one\" + 0.007*\"bitch\"\n",
      "Topic: 2 \n",
      "Words: 0.013*\"wanna\" + 0.011*\"baby\" + 0.011*\"gonna\" + 0.010*\"back\" + 0.010*\"night\" + 0.010*\"never\" + 0.010*\"way\" + 0.009*\"make\" + 0.009*\"long\" + 0.008*\"time\"\n",
      "Topic: 3 \n",
      "Words: 0.016*\"one\" + 0.013*\"oh\" + 0.012*\"let\" + 0.011*\"way\" + 0.008*\"time\" + 0.008*\"go\" + 0.008*\"girl\" + 0.008*\"gonna\" + 0.008*\"make\" + 0.007*\"say\"\n",
      "Topic: 4 \n",
      "Words: 0.017*\"girl\" + 0.010*\"say\" + 0.010*\"good\" + 0.010*\"time\" + 0.010*\"go\" + 0.009*\"right\" + 0.009*\"ooh\" + 0.009*\"one\" + 0.009*\"make\" + 0.009*\"heart\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(num_topics=-1, num_words=n_top_words):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.007*\"bitch\" + 0.004*\"nigga\" + 0.004*\"fuck\" + 0.004*\"let\" + 0.003*\"want\" + 0.003*\"oh\" + 0.003*\"good\" + 0.003*\"goodbye\" + 0.003*\"baby\" + 0.003*\"nasty\"\n",
      "Topic: 1 \n",
      "Word: 0.004*\"wanna\" + 0.004*\"baby\" + 0.004*\"make\" + 0.003*\"country\" + 0.003*\"never\" + 0.003*\"stand\" + 0.003*\"la\" + 0.003*\"oh\" + 0.003*\"life\" + 0.003*\"gonna\"\n",
      "Topic: 2 \n",
      "Word: 0.005*\"girl\" + 0.004*\"wanna\" + 0.004*\"oh\" + 0.004*\"gonna\" + 0.003*\"heart\" + 0.003*\"little\" + 0.003*\"could\" + 0.003*\"one\" + 0.003*\"home\" + 0.003*\"make\"\n",
      "Topic: 3 \n",
      "Word: 0.004*\"still\" + 0.003*\"need\" + 0.003*\"oh\" + 0.003*\"baby\" + 0.003*\"let\" + 0.003*\"heart\" + 0.003*\"every\" + 0.003*\"time\" + 0.003*\"call\" + 0.003*\"go\"\n",
      "Topic: 4 \n",
      "Word: 0.004*\"baby\" + 0.004*\"want\" + 0.004*\"wanna\" + 0.004*\"always\" + 0.004*\"never\" + 0.003*\"gonna\" + 0.003*\"would\" + 0.003*\"girl\" + 0.003*\"say\" + 0.003*\"little\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = models.LdaMulticore(corpus_tfidf, num_topics=n_topics, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(num_topics=-1, num_words=n_top_words):\n",
    "    print('Topic: {} \\nWord: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d76e4664c33a11e40f25fff09cff43d7ba989e17a7602e4df3d22dcb6e2efdae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('MADS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
